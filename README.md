# Автоматизированная система анализа просроченных кредитов

### Цель проекта
- Спроектировать DWH с таблицами Loans (ID, клиент, сумма, срок, статус) и Clients (ID, имя).
- Реализовать ETL для загрузки JSON, пометить кредиты как просроченные (дата > срока), вычислить процент просрочки.
- Создать BI-витрину: график доли просрочек по месяцам, фильтр по сумме.
- Провести тестирование на 10 записях.
- Предложить оптимизацию процесса уведомления клиентов.
- Усложнение: вычисления и тестирование.

### Исходные данные проекта

- таблица Clients: содержит подробную информацию о каждом клиенте;
- таблица Loans: содержит информацию о кредитах клиентов;
- таблица Payments: содержит историчную информацию о платежах клиентов.

### Хранилище данных
Система предусматривает хранение данных в едином хранилище данных (ЕХД), разделенном на несколько слоев:
- слой staging - здесь хранятся «сырые» загрузки из JSON-файлов, хранящих информацию о клиентах, их кредитах и истории платежей;
- слой core - здесь хранятся данные, преобразованные со слоя staging без затирания;
- слой mart - здесь формируется витрина для BI — агрегированные и подготовленные для отчёта данные.

Код написан в учебных целях — это практическое задание в курсе "Системный аналитик" от Лиги Цифровых Экономик.


# Пример финального предоставления отчета через Power BI
https://github.com/user-attachments/assets/37825e6b-4d0b-42bb-bd5e-7ce9e21fc9e3


# Подготовительные операции

### 1. Копируем содержимое проекта себе в рабочую директорию
``` Python
git clone https://github.com/BgKr-97/LigaProject.git
```

### 2. Устанавливаем библиотеки
``` Python
pip install -r requirements.txt
```

### 3. Для хранения переменных окружения создаем файл .env
В файл .env записываем пути до файлов с исходными данными, а также параметры для подключения к вашей БД (базе данных).

**Обозначения:**

`START_LOAN_DATE` - минимальная дата открытия кредита

`RAW_DIR` - папка для хранения "сырого" источника данных

`SPLIT_DIR` - папка для хранения "сырого" источника данных, разделенного на несколько частей

``` txt
# Подключение к БД
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASS=***
DB_NAME=postgres

# Папки с данными
RAW_DIR=data_generation/raw_files
SPLIT_DIR=data_generation/raw_split_files

# Настройки генерации данных
START_LOAN_DATE=2010-01-01
```

### 4. Создание структуры DWH:
В проекте реализован автоматический механизм создания структуры БД - через исполнение SQL-скриптов. Вызов данных скриптов автоматизирован - он осуществляется через обработку аргументов командной строки с помощью `argparse`. То есть, чтобы запустить процесс инициализации БД, достаточно выполнить соответствующую команду:
``` Python
python main.py shema
```
В ответ на этот аргумент код сам найдет и выполнит нужные SQL-файлы, создав все схемы и таблицы в нужном порядке.

## Генерация/загрузка/преобразование данных
### 1. Генерация данных
В проекте реализована инкрементная загрузка данных в БД, а не полным дампом — это стандартный и осознанный подход в продакшн-системах.
1. Указав параметры для генерации данных, команда 
``` Python
python main.py generate
```
сгенерирует "сырые" данные в виде трех JSON-файлов (clients.json, loans.json и payments.json) и сохранит их в указанную в файле `.env` папку. По умолчанию генерируется 20 клиентов.

Задав дополнительный параметр `--num-clients`, команда `generate` сгенерирует необходимое количество клиентов. 
Например, команда
``` Python
python main.py generate --num-clients 100
```
сгенерирует JSON-файлы, в которых будет учтено 100 клиентов и соответствующие им операции.

2. Команда 
``` Python
python main.py split
```
разделит существующие JSON-файлы на n частей (по умолчанию `n = 5`), и сохранит их в указанную в файле `.env` папку.
Разделение происходит *кумулятивно*, то есть каждый последующий файл содержит в себе информацию предыдущей части. 
Это необходимо для реализации метода UPSERT, предназначенного для поддержания целостности данных и избежания дубликатов записей.

Задав дополнительный параметр `--parts`, команда `split` разделит исходные JSON-файлы на необходимое количество частей. Например, команда
``` Python
python main.py split --parts 10
```
разделит исходные JSON-файлы на 10 частей. Разделенные JSON-файлы будут храниться в указанной в файле `.env` папке с указанием номера части.

### 2. Загрузка данных
Перевод данных с одного слоя на другой осуществляется посредством автоматического запуска ETL процессов в целях упрощения взаимодействия с системой. 
Вызов команды `load` 
``` Python
python main.py load --part N
```
инкрементально загружает одну часть данных (файлы clients_N.json, loans_N.json и payments_N.json, где N - номер части) сразу во все слои БД:
- слой staging: сюда данные попадают с сохранением своего первоначального вида и с указанием timestamp записи, а также источника получения данных. Уникальность записей проверяется по отдельным полям;
- слой core: преобразованные со слоя staging данные попадают сюда без затирания. При загрузке данных на этот слой добавляется булево поле `status`, показывающее просроченную операцию клиентов по их кредитам. Уникальность записей проверяется по отдельным полям;
- слой mart: сюда данные попадают со слоя core в денормализованном виде. Заранее подготовленное представление (VIEW) на данном слое служит источником для демонстрации данных в Power BI.

Задав дополнительный параметр `--all`, команда `load` загрузит в БД сразу все части данных:
``` Python
python main.py load --all
```

**Предпочтительный** способ загрузки данных - инкрементный (по частям). Данный способ дает возможность ощутить периодическое обновление данных, как в реальных проектах.

# Пример процесса загрузки данных


https://github.com/user-attachments/assets/7af5cba5-ea88-4d10-8d64-a5c2f46d0322



# Демонстрация данных
Демонстрация данных производится на основании PowerBI из таблицы data_mart, находящейся на слое mart, а также из представления (VIEW) vw_overdue_by_month_and_amount:

Связываем PowerBI с БД с таблицей data_mart и преставлением vw_overdue_by_month_and_amount. При очередной загрузке порции данных необходимо обновить источники в PowerBI.

Таблица data_mart используется в качестве сводки данных по клиентам для просмотра просроченных кредитов. 

Представление vw_overdue_by_month_and_amount - для KPI карточек и построения графика долей просрочки кредитов с фильтрацией по сумме кредита.


Пример отчета находится в файле "Отчет.pbix".


# Дополнительные выводы
На основе витрины данных о кредитах клиентов (data_mart) можно построить зависимости между различными переменными, чтобы выявить закономерности и взаимосвязи.
В качестве дополнительных выводов о портрете "типичного" должника, воспользуемся снимком данных о просроченных платежах клиентов за 2023 год и проанализируем часть информации.

![image](https://github.com/user-attachments/assets/a8fcdc6f-887f-4779-b992-128e5eb07db9)

На снимке видно, что лидерами по количеству имеющихся у клиентов просрочек по кредитам являются Новосибирск, Санкт-Петербург и Нижний Новгород. 
По полу распределение почти равное, с небольшим перевесом у женщин. По типу занятости преобладают клиенты, работающие по найму, за ними - самозанятые, а ИП и безработные составляют меньшинство.

На перспективу на основе витрины (data_mart) можно исследовать зависимости между суммой кредита, доходом, типом занятости, просрочками, полом, возрастом, регионом и другими факторами. Для более глубокого анализа можно использовать статистические методы, такие как корреляция, регрессия или кластеризация, чтобы выявить скрытые закономерности.





